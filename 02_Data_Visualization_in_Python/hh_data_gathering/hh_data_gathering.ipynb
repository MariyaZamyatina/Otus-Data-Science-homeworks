{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сбор данных с сайта hh.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install beautifulsoup4 lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружались вакансии за последний месяц по ключенвым словам: data science, deep learning и т.д.\n",
    "Из скачанных данных были отобраны наиболее интересные поля. Сбор данных был разделен на 2 этапа из-за ограничения API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data  gathering.py  hh_data_gathering.ipynb  parsers  scrappers  storages\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Поиск и загрузка открытых вакансий**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --search - строка поиска вакансий\n",
    "# --pages - число страниц\n",
    "# число вакансий на странице, max = 100 - ограничение API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Work started\n",
      "INFO:__main__:Gather process\n",
      "INFO:scrappers.hh_scrapper:Getting data from hh.ru...\n",
      "INFO:scrappers.hh_scrapper:search_text: !\"data science\" OR !\"data scientist\" OR !\"machine learning\" OR !\"Deep Learning\" OR !\"Data Analysis\"\n",
      "INFO:scrappers.hh_scrapper:per_page: 100\n",
      "INFO:scrappers.hh_scrapper:pages: 20\n",
      "100%|███████████████████████████████████████| 1222/1222 [02:06<00:00,  9.64it/s]\n",
      "INFO:scrappers.hh_scrapper:Save 1222 vacancies from hh.ru to the data/hh_scrapped_data_part1.txt file\n",
      "INFO:__main__:Work ended\n"
     ]
    }
   ],
   "source": [
    "!python3 -m gathering gather --search '!\"data science\" OR !\"data scientist\" OR !\"machine learning\" OR !\"Deep Learning\" OR !\"Data Analysis\"' \\\n",
    "--pages 20 --file 'data/hh_scrapped_data_part1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Work started\n",
      "INFO:__main__:Gather process\n",
      "INFO:scrappers.hh_scrapper:Getting data from hh.ru...\n",
      "INFO:scrappers.hh_scrapper:search_text: !\"машинное обучение\" OR !\"анализ данных\" OR !\"нейронные сети\"\n",
      "INFO:scrappers.hh_scrapper:per_page: 100\n",
      "INFO:scrappers.hh_scrapper:pages: 20\n",
      "100%|███████████████████████████████████████| 2000/2000 [03:13<00:00, 10.36it/s]\n",
      "INFO:scrappers.hh_scrapper:Save 2000 vacancies from hh.ru to the data/hh_scrapped_data_part2.txt file\n",
      "INFO:__main__:Work ended\n"
     ]
    }
   ],
   "source": [
    "!python3 -m gathering gather --search '!\"машинное обучение\" OR !\"анализ данных\" OR !\"нейронные сети\"' \\\n",
    "--pages 20 --file 'data/hh_scrapped_data_part2.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Преобразование загруженных данных в табличный формат**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Work started\n",
      "INFO:__main__:transform\n",
      "INFO:parsers.hh_parser:Transformed data saved to the data/hh_data_part1.csv file\n",
      "INFO:__main__:Work ended\n"
     ]
    }
   ],
   "source": [
    "!python3 -m gathering transform --filefrom 'data/hh_scrapped_data_part1.txt' --fileto 'data/hh_data_part1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Work started\n",
      "INFO:__main__:transform\n",
      "INFO:parsers.hh_parser:Transformed data saved to the data/hh_data_part2.csv file\n",
      "INFO:__main__:Work ended\n"
     ]
    }
   ],
   "source": [
    "!python3 -m gathering transform --filefrom 'data/hh_scrapped_data_part2.txt' --fileto 'data/hh_data_part2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 88M\r\n",
      "-rw-rw-r-- 1 mariya mariya 4,4M сен 30 20:52 hh_data_part1.csv\r\n",
      "-rw-rw-r-- 1 mariya mariya 6,8M сен 30 20:52 hh_data_part2.csv\r\n",
      "-rw-rw-r-- 1 mariya mariya  32M сен 30 20:45 hh_scrapped_data_part1.txt\r\n",
      "-rw-rw-r-- 1 mariya mariya  45M сен 30 20:50 hh_scrapped_data_part2.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Объединение загруженных данных**\n",
    "\n",
    "Удаление вакансий-дубликатов. Сохранение объединенных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data/hh_data_part1.csv', sep='\\t')\n",
    "df2 = pd.read_csv('data/hh_data_part2.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1222, 22)\n",
      "(2000, 22)\n"
     ]
    }
   ],
   "source": [
    "print(df1.shape)\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2960, 22)\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([df1, df2])\n",
    "df.drop_duplicates(['id'], inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/hh_data.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 98M\r\n",
      "-rw-rw-r-- 1 mariya mariya  11M сен 30 20:55 hh_data.csv\r\n",
      "-rw-rw-r-- 1 mariya mariya 4,4M сен 30 20:52 hh_data_part1.csv\r\n",
      "-rw-rw-r-- 1 mariya mariya 6,8M сен 30 20:52 hh_data_part2.csv\r\n",
      "-rw-rw-r-- 1 mariya mariya  32M сен 30 20:45 hh_scrapped_data_part1.txt\r\n",
      "-rw-rw-r-- 1 mariya mariya  45M сен 30 20:50 hh_scrapped_data_part2.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
